{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87eef0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [00:00<00:00, 24564.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished. Q-size: 13822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "# Environment\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.board = [0]*9\n",
    "        self.current_player = 1\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        return tuple(self.board)\n",
    "    def available_actions(self):\n",
    "        return [i for i,v in enumerate(self.board) if v==0]\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            raise ValueError('Game already finished')\n",
    "        if self.board[action] != 0:\n",
    "            self.done = True\n",
    "            self.winner = -self.current_player\n",
    "            reward = -1.0 if self.current_player==1 else 1.0\n",
    "            return tuple(self.board), reward, self.done, {}\n",
    "        self.board[action] = self.current_player\n",
    "        lines = [(0,1,2),(3,4,5),(6,7,8),(0,3,6),(1,4,7),(2,5,8),(0,4,8),(2,4,6)]\n",
    "        for (i,j,k) in lines:\n",
    "            s = self.board[i]+self.board[j]+self.board[k]\n",
    "            if s == 3:\n",
    "                self.done = True\n",
    "                self.winner = 1\n",
    "                return tuple(self.board), 1.0, True, {}\n",
    "            if s == -3:\n",
    "                self.done = True\n",
    "                self.winner = -1\n",
    "                return tuple(self.board), -1.0, True, {}\n",
    "        if all(v!=0 for v in self.board):\n",
    "            self.done = True\n",
    "            self.winner = 0\n",
    "            return tuple(self.board), 0.5, True, {}\n",
    "        self.current_player *= -1\n",
    "        return tuple(self.board), 0.0, False, {}\n",
    "\n",
    "# Agent\n",
    "class QAgent:\n",
    "    def __init__(self, alpha=0.5, gamma=0.9, epsilon=0.1):\n",
    "        self.Q = defaultdict(float)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "    def get_Q(self, state, action):\n",
    "        return self.Q[(state, action)]\n",
    "    def choose_action(self, state, available_actions, training=True):\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.choice(available_actions)\n",
    "        qvals = [self.get_Q(state, a) for a in available_actions]\n",
    "        max_q = max(qvals)\n",
    "        max_actions = [a for a,q in zip(available_actions, qvals) if q==max_q]\n",
    "        return random.choice(max_actions)\n",
    "    def update(self, state, action, reward, next_state, next_available_actions, done):\n",
    "        cur = self.get_Q(state, action)\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            future_qs = [self.get_Q(next_state, a) for a in next_available_actions] if next_available_actions else [0.0]\n",
    "            target = reward + self.gamma * max(future_qs)\n",
    "        self.Q[(state, action)] = cur + self.alpha * (target - cur)\n",
    "\n",
    "def train(agent, episodes=20000, opponent='random', verbose=False):\n",
    "    env = TicTacToe()\n",
    "    stats = {'wins':0, 'losses':0, 'draws':0}\n",
    "    history = {'wins':[], 'losses':[], 'draws':[]}\n",
    "    report_every = max(1, episodes//50)\n",
    "    for ep in trange(episodes, desc='Training'):\n",
    "        state = env.reset()\n",
    "        if random.random() < 0.5:\n",
    "            env.current_player = 1\n",
    "        else:\n",
    "            env.current_player = -1\n",
    "        done = False\n",
    "        while not done:\n",
    "            if env.current_player == 1:\n",
    "                avail = env.available_actions()\n",
    "                action = agent.choose_action(state, avail, training=True)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                if done:\n",
    "                    agent.update(state, action, reward, next_state, [], done=True)\n",
    "                    if reward == 1.0:\n",
    "                        stats['wins'] += 1\n",
    "                    elif reward == -1.0:\n",
    "                        stats['losses'] += 1\n",
    "                    else:\n",
    "                        stats['draws'] += 1\n",
    "                else:\n",
    "                    opp_avail = env.available_actions()\n",
    "                    opp_action = random.choice(opp_avail)\n",
    "                    next_state2, reward2, done2, _ = env.step(opp_action)\n",
    "                    if done2:\n",
    "                        if reward2 == -1.0:\n",
    "                            agent.update(state, action, -1.0, next_state2, [], done=True)\n",
    "                            stats['losses'] += 1\n",
    "                        elif reward2 == 1.0:\n",
    "                            agent.update(state, action, 1.0, next_state2, [], done=True)\n",
    "                            stats['wins'] += 1\n",
    "                        else:\n",
    "                            agent.update(state, action, 0.5, next_state2, [], done=True)\n",
    "                            stats['draws'] += 1\n",
    "                        done = True\n",
    "                    else:\n",
    "                        next_avail = env.available_actions()\n",
    "                        agent.update(state, action, 0.0, next_state2, next_avail, done=False)\n",
    "                        state = next_state2\n",
    "            else:\n",
    "                avail = env.available_actions()\n",
    "                action = random.choice(avail)\n",
    "                state_after, reward_after, done, _ = env.step(action)\n",
    "                if done:\n",
    "                    if reward_after == -1.0:\n",
    "                        stats['losses'] += 1\n",
    "                    elif reward_after == 1.0:\n",
    "                        stats['wins'] += 1\n",
    "                    else:\n",
    "                        stats['draws'] += 1\n",
    "                    break\n",
    "                else:\n",
    "                    state = state_after\n",
    "        if (ep+1) % report_every == 0:\n",
    "            history['wins'].append(stats['wins'])\n",
    "            history['losses'].append(stats['losses'])\n",
    "            history['draws'].append(stats['draws'])\n",
    "            if verbose:\n",
    "                print('Episode {}: W/L/D = {}/{}/{}'.format(ep+1, stats['wins'], stats['losses'], stats['draws']))\n",
    "    return agent, history\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    agent = QAgent(alpha=0.5, gamma=0.9, epsilon=0.2)\n",
    "    agent, history = train(agent, episodes=20000, opponent='random', verbose=False)\n",
    "    print('Training finished. Q-size:', len(agent.Q))\n",
    "    wins, losses, draws = evaluate(agent, games=2000, opponent='random') if 'evaluate' in globals() else (None, None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f44b5d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ® Welcome to Tic-Tac-Toe!\n",
      "Positions are numbered 1â€“9 as follows:\n",
      " 1 | 2 | 3\n",
      " 4 | 5 | 6\n",
      " 7 | 8 | 9\n",
      "\n",
      "You are 'O' (-1). The AI is 'X' (+1). Let's play!\n",
      "\n",
      "\n",
      "ðŸ¤– AI played at position 5\n",
      "\n",
      "Board:\n",
      "   |   |   \n",
      "---+---+---\n",
      "   | X |   \n",
      "---+---+---\n",
      "   |   |   \n",
      "\n",
      "\n",
      "Board:\n",
      "   |   |   \n",
      "---+---+---\n",
      "   | X |   \n",
      "---+---+---\n",
      "   |   |   \n",
      "\n",
      "\n",
      "ðŸ¤– AI played at position 3\n",
      "\n",
      "Board:\n",
      " O |   | X \n",
      "---+---+---\n",
      "   | X |   \n",
      "---+---+---\n",
      "   |   |   \n",
      "\n",
      "\n",
      "Board:\n",
      " O |   | X \n",
      "---+---+---\n",
      "   | X |   \n",
      "---+---+---\n",
      "   |   |   \n",
      "\n",
      "\n",
      "ðŸ¤– AI played at position 2\n",
      "\n",
      "Board:\n",
      " O | X | X \n",
      "---+---+---\n",
      "   | X |   \n",
      "---+---+---\n",
      " O |   |   \n",
      "\n",
      "\n",
      "Board:\n",
      " O | X | X \n",
      "---+---+---\n",
      "   | X |   \n",
      "---+---+---\n",
      " O |   |   \n",
      "\n",
      "\n",
      "ðŸ¤– AI played at position 4\n",
      "\n",
      "Board:\n",
      " O | X | X \n",
      "---+---+---\n",
      " X | X |   \n",
      "---+---+---\n",
      " O | O |   \n",
      "\n",
      "\n",
      "Board:\n",
      " O | X | X \n",
      "---+---+---\n",
      " X | X |   \n",
      "---+---+---\n",
      " O | O |   \n",
      "\n",
      "ðŸŽ‰ You win!\n"
     ]
    }
   ],
   "source": [
    "def print_board(board):\n",
    "    symbols = {1: 'X', -1: 'O', 0: ' '}\n",
    "    print(\"\\nBoard:\")\n",
    "    for i in range(0, 9, 3):\n",
    "        print(f\" {symbols[board[i]]} | {symbols[board[i+1]]} | {symbols[board[i+2]]} \")\n",
    "        if i < 6:\n",
    "            print(\"---+---+---\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def play_against_agent(agent, human_starts=True):\n",
    "    env = TicTacToe()\n",
    "    state = env.reset()\n",
    "    print(\"ðŸŽ® Welcome to Tic-Tac-Toe!\")\n",
    "    print(\"Positions are numbered 1â€“9 as follows:\")\n",
    "    print(\" 1 | 2 | 3\\n 4 | 5 | 6\\n 7 | 8 | 9\\n\")\n",
    "    print(\"You are 'O' (-1). The AI is 'X' (+1). Let's play!\\n\")\n",
    "\n",
    "    while not env.done:\n",
    "        # Human's turn\n",
    "        if (env.current_player == -1 and human_starts) or (env.current_player == 1 and not human_starts):\n",
    "            print_board(env.board)\n",
    "            move = input(\"Enter your move (1â€“9): \")\n",
    "\n",
    "            # Validate move\n",
    "            if not move.isdigit():\n",
    "                print(\"âŒ Invalid input. Enter a number between 1 and 9.\")\n",
    "                continue\n",
    "\n",
    "            move = int(move) - 1  # Convert to 0-based index\n",
    "\n",
    "            if move not in env.available_actions():\n",
    "                print(\"âŒ That cell is already taken or invalid. Try again.\")\n",
    "                continue\n",
    "\n",
    "            state, reward, done, _ = env.step(move)\n",
    "\n",
    "        # AI's turn\n",
    "        else:\n",
    "            avail = env.available_actions()\n",
    "            action = agent.choose_action(state, avail, training=False)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            print(f\"\\nðŸ¤– AI played at position {action + 1}\")\n",
    "            print_board(env.board)\n",
    "\n",
    "        # Check game result\n",
    "        if done:\n",
    "            if env.winner == 1:\n",
    "                print(\"ðŸ’» AI wins!\")\n",
    "            elif env.winner == -1:\n",
    "                print(\"ðŸŽ‰ You win!\")\n",
    "            else:\n",
    "                print(\"ðŸ¤ It's a draw!\")\n",
    "            break\n",
    "\n",
    "\n",
    "# --- Run this after training ---\n",
    "play_against_agent(agent, human_starts=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
